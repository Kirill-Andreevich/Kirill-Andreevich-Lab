---
- name: ğŸš€ Deploy Optimized Qwen 2.5 Coder with Web Search
  hosts: nvidia_gpu
  become: true
  vars:
    model_name: "qwen2.5-coder:32b"
    fast_model_name: "qwen-fast"
    docker_network_name: ai_network
    ollama_container_name: ollama
    open_webui_container_name: open-webui
    searxng_container_name: searxng
    ollama_image: ollama/ollama:latest
    open_webui_image: ghcr.io/open-webui/open-webui:main
    searxng_image: searxng/searxng:latest
    ollama_storage_volume: ollama_storage
    open_webui_data_volume: open-webui-data
    searxng_config_dir: "/opt/searxng"

  tasks:
    - name: ğŸ›  Prepare Environment
      block:
        - name: ğŸ“¦ Install Python Docker SDK
          community.general.pacman:
            name: python-docker
            state: present
        - name: ğŸŒ Create Docker Network
          community.docker.docker_network:
            name: "{{ docker_network_name }}"
        - name: ğŸ“ Create SearXNG Config Directory
          file:
            path: "{{ searxng_config_dir }}"
            state: directory
            mode: '0755'

    - name: ğŸ“ Generate SearXNG Settings (Enable JSON & API)
      copy:
        dest: "{{ searxng_config_dir }}/settings.yml"
        content: |
          use_default_settings: true
          server:
            secret_key: "ai_search_secret"
            limiter: false
            image_proxy: true
          search:
            formats:
              - html
              - json
          enabled_plugins:
            - 'Hash plugin'
            - 'Self-Informative'

    - name: ğŸ“¦ Install NVIDIA Container Toolkit
      community.general.pacman:
        name: nvidia-container-toolkit
        state: present

    - name: ğŸ”„ Restart Docker Service
      systemd:
        name: docker
        state: restarted
        enabled: true

    - name: ğŸ³ Run Ollama Container (RTX 4090)
      community.docker.docker_container:
        name: "{{ ollama_container_name }}"
        image: "{{ ollama_image }}"
        state: started
        restart_policy: always
        networks:
          - name: "{{ docker_network_name }}"
        published_ports:
          - "11434:11434"
        volumes:
          - "{{ ollama_storage_volume }}:/root/.ollama"
        device_requests:
          - driver: nvidia
            count: -1
            capabilities: [["gpu"]]

    - name: ğŸ” Run SearXNG (Search Engine)
      community.docker.docker_container:
        name: "{{ searxng_container_name }}"
        image: "{{ searxng_image }}"
        state: started
        restart_policy: always
        networks:
          - name: "{{ docker_network_name }}"
        volumes:
          - "{{ searxng_config_dir }}/settings.yml:/etc/searxng/settings.yml:ro"

    - name: ğŸ–¥ Run Open WebUI with Web Search
      community.docker.docker_container:
        name: "{{ open_webui_container_name }}"
        image: "{{ open_webui_image }}"
        state: started
        restart_policy: always
        networks:
          - name: "{{ docker_network_name }}"
        published_ports:
          - "3000:8080"
        volumes:
          - "{{ open_webui_data_volume }}:/app/backend/data"
        env:
          OLLAMA_BASE_URL: "http://{{ ollama_container_name }}:11434"
          WEBUI_AUTH: "False"
          ENABLE_RAG_WEB_SEARCH: "True"
          RAG_WEB_SEARCH_ENGINE: "searxng"
          # ĞŸĞ»ĞµĞ¹ÑÑ…Ğ¾Ğ»Ğ´ĞµÑ€ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° <query> Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Open WebUI
          SEARXNG_QUERY_URL: "http://{{ searxng_container_name }}:8080/search?q=<query>"
          RAG_WEB_SEARCH_RESULT_COUNT: "3"

    - name: ğŸ§  Pull Base Model
      command: "docker exec {{ ollama_container_name }} ollama pull {{ model_name }}"
      async: 1800
      poll: 30

    - name: âš™ Create Fast Model (Optimized for 4090)
      shell: |
        docker exec {{ ollama_container_name }} sh -c "echo 'FROM {{ model_name }}\nPARAMETER num_ctx 2048\nPARAMETER temperature 0.7' > /tmp/Modelfile"
        docker exec {{ ollama_container_name }} ollama create {{ fast_model_name }} -f /tmp/Modelfile
      changed_when: true

    - name: âœ¨ Final Report
      debug:
        msg: 
          - "========================================================="
          - "  AI STACK READY!"
          - "  URL: http://{{ ansible_host }}:3000"
          - "  Model: {{ fast_model_name }} (Optimized for Speed)"
          - "  Web Search: JSON Format Enabled & Verified"
          - "========================================================="
    - name: ğŸ“ Create Chat Bot Python Script
      copy:
        dest: "/tmp/qwen_app.py"
        content: |
          import streamlit as st
          import requests
          import json
          st.set_page_config(page_title="Qwen 2.5 Coder 4090", layout="wide")
          st.title("ğŸš€ Qwen 2.5 Coder (32B) on 4090")
          if "messages" not in st.session_state:
              st.session_state.messages = []
          for message in st.session_state.messages:
              with st.chat_message(message["role"]):
                  st.markdown(message["content"])
          if prompt := st.chat_input("ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ ĞºĞ¾Ğ´..."):
              st.session_state.messages.append({"role": "user", "content": prompt})
              with st.chat_message("user"):
                  st.markdown(prompt)
              with st.chat_message("assistant"):
                  response_placeholder = st.empty()
                  full_response = ""
                  res = requests.post(
                      "http://ollama:11434/api/generate",
                      json={"model": "qwen-fast", "prompt": prompt, "stream": True},
                      stream=True
                  )
                  for line in res.iter_lines():
                      if line:
                          chunk = json.loads(line.decode('utf-8'))
                          full_response += chunk.get("response", "")
                          response_placeholder.markdown(full_response + "â–Œ")
                  response_placeholder.markdown(full_response)
              st.session_state.messages.append({"role": "assistant", "content": full_response})

    - name: ğŸ¤– Run Streamlit Chat Bot Container
      community.docker.docker_container:
        name: "qwen-chat-bot"
        image: "python:3.11-slim" 
        state: started
        restart_policy: always
        networks:
          - name: "ai_network"
        published_ports:
          - "5000:8501"
        volumes:
          - "/tmp/qwen_app.py:/app/app.py"
        # Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹
        command: >
          sh -c "pip install streamlit requests && streamlit run /app/app.py --server.address 0.0.0.0"
